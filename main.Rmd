```{r}
library(tidyverse) 
library(dplyr)
library(ggplot2)
library(corrplot)
library(missForest)
library(mgcv)
library(caret)
library(ranger)
library(xgboost)
```

```{r}
rmse <- function(actual, predicted) {
  # Stelle sicher, dass 'actual' und 'predicted' gleich lang sind
  stopifnot(length(actual) == length(predicted))
  
  # Berechne den Mean Squared Error
  mse <- mean((actual - predicted)^2, na.rm = TRUE)
  
  # Nimm die Quadratwurzel, um den RMSE zu erhalten
  sqrt(mse)
}
```

# Data Prep
## Train
```{r}
train <- readr::read_csv("train.csv", show_col_types = FALSE)
train <- train %>% mutate(Podcast_Name = as.factor(train$Podcast_Name),
                Episode_Title = as.factor(train$Episode_Title),
                Genre = as.factor(train$Genre),
                Publication_Day	 = as.factor(train$Publication_Day),
                Publication_Time = as.factor(train$Publication_Time),
                Episode_Sentiment = as.factor(train$Episode_Sentiment)) 
colSums(is.na(train))
str(train)

#Train imputation

train <- train %>%
  mutate(
    Episode_Length_minutes = if_else(
      is.na(Episode_Length_minutes), 
      median(train$Episode_Length_minutes, na.rm = TRUE), 
      Episode_Length_minutes
    ),
    Guest_Popularity_percentage = if_else(
      is.na(Guest_Popularity_percentage),
      median(train$Guest_Popularity_percentage, na.rm = TRUE),
      Guest_Popularity_percentage
    ),
    Number_of_Ads = if_else(
      is.na(Number_of_Ads), 
      median(train$Number_of_Ads, na.rm = TRUE), 
      Number_of_Ads
    )
  )
```
## Test
```{r}
test <- readr::read_csv("test.csv", show_col_types = FALSE)
test <- test %>% mutate(Podcast_Name = as.factor(test$Podcast_Name),
                Episode_Title = as.factor(test$Episode_Title),
                Genre = as.factor(test$Genre),
                Publication_Day	 = as.factor(test$Publication_Day),
                Publication_Time = as.factor(test$Publication_Time),
                Episode_Sentiment = as.factor(test$Episode_Sentiment)) 
colSums(is.na(test))
str(test)

# #Test imputation Median
# test <- test %>%
#   mutate(
#     Episode_Length_minutes = if_else(
#       is.na(Episode_Length_minutes), 
#       median(test$Episode_Length_minutes, na.rm = TRUE), 
#       Episode_Length_minutes
#     ),
#     Guest_Popularity_percentage = if_else(
#       is.na(Guest_Popularity_percentage),
#       median(test$Guest_Popularity_percentage, na.rm = TRUE),
#       Guest_Popularity_percentage
#     )
#   )
```



## Viz
```{r}
train %>%
  ggplot( aes(x=Listening_Time_minutes)) +
    geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) 
```

```{r}
cor <- train %>% select(Episode_Length_minutes, Host_Popularity_percentage, Guest_Popularity_percentage, Number_of_Ads, Listening_Time_minutes) %>% na.omit()
M = cor(cor)
corrplot(M, method = 'number') # colorful number
```


# Training

## LM
```{r}
set.seed(123)     # Für reproduzierbare Zufallsergebnisse
K <- 5            # Anzahl der Folds
N <- nrow(train)

# 1. Erstelle einen Zufalls-Vektor, der jedem Datenpunkt 
#    einen Fold (1..K) zuweist:
folds <- sample(rep(1:K, length.out = N))

# 2. Crossvalidation-Schleife
cv_rmse <- numeric(K)  # Hier sammeln wir die RMSE-Werte pro Fold

# Variablen zum Speichern des "besten" Modells und seines RMSE:
best_model <- NULL
best_rmse  <- Inf

for (i in 1:K) {
  # Trainings- und Testindices
  train_ind <- which(folds != i)
  test_ind  <- which(folds == i)
  
  # Trainings- und Testdatensätze
  train_data <- train[train_ind, ]
  test_data  <- train[test_ind, ]
  
  # GAM-Modell fitten (einfaches Beispiel)
  model <- lm(Listening_Time_minutes ~ Episode_Length_minutes,
    data = train_data
  )
  
  # Vorhersage auf Testdaten
  preds <- predict(model, newdata = test_data)
  
  # RMSE für diesen Fold
  actuals <- test_data$Listening_Time_minutes
  rmse    <- sqrt(mean((actuals - preds)^2))
  cv_rmse[i] <- rmse
  
  # Prüfen, ob dieses Modell besser ist als das bisher beste
  if(rmse < best_rmse) {
    best_rmse  <- rmse
    best_model <- model
  }
}

# 3. Durchschnittlicher RMSE über alle Folds
mean_cv_rmse <- mean(cv_rmse)
mean_cv_rmse

# 'best_model' enthält jetzt das Modell mit dem geringsten RMSE 
# in seinem jeweiligen Fold.
summary(best_model)

```


## GAM
```{r}
set.seed(123)     # Für reproduzierbare Zufallsergebnisse
K <- 5            # Anzahl der Folds
N <- nrow(train)

# 1. Erstelle einen Zufalls-Vektor, der jedem Datenpunkt 
#    einen Fold (1..K) zuweist:
folds <- sample(rep(1:K, length.out = N))

# 2. Crossvalidation-Schleife
cv_rmse <- numeric(K)  # Hier sammeln wir die RMSE-Werte pro Fold

# Variablen zum Speichern des "besten" Modells und seines RMSE:
best_model <- NULL
best_rmse  <- Inf

for (i in 1:K) {
  # Trainings- und Testindices
  train_ind <- which(folds != i)
  test_ind  <- which(folds == i)
  
  # Trainings- und Testdatensätze
  train_data <- train[train_ind, ]
  test_data  <- train[test_ind, ]
  
  # GAM-Modell fitten (einfaches Beispiel)
  model <- mgcv::gam(
    Listening_Time_minutes ~ s(Episode_Length_minutes) ,
    family = gaussian(),
    data = train_data,
    method = "REML"
  )
  
  #s(Podcast_Name, bs = "re")
  
  # Vorhersage auf Testdaten
  preds <- predict(model, newdata = test_data)
  
  # RMSE für diesen Fold
  actuals <- test_data$Listening_Time_minutes
  rmse    <- sqrt(mean((actuals - preds)^2))
  cv_rmse[i] <- rmse
  
  # Prüfen, ob dieses Modell besser ist als das bisher beste
  if(rmse < best_rmse) {
    best_rmse  <- rmse
    best_model <- model
  }
}

# 3. Durchschnittlicher RMSE über alle Folds
mean_cv_rmse <- mean(cv_rmse)
mean_cv_rmse

# 'best_model' enthält jetzt das Modell mit dem geringsten RMSE 
# in seinem jeweiligen Fold.
summary(best_model)


```


## Random Forest
```{r}
set.seed(123)     # Für reproduzierbare Zufallsergebnisse
K <- 5            # Anzahl der Folds
N <- nrow(train)

# 1. Erstelle einen Zufalls-Vektor, der jedem Datenpunkt 
#    einen Fold (1..K) zuweist:
folds <- sample(rep(1:K, length.out = N))

# 2. Crossvalidation-Schleife
cv_rmse <- numeric(K)  # Hier sammeln wir die RMSE-Werte pro Fold

# Variablen zum Speichern des "besten" Modells und seines RMSE:
best_model <- NULL
best_rmse  <- Inf

for (i in 1:K) {
  # Trainings- und Testindices
  train_ind <- which(folds != i)
  test_ind  <- which(folds == i)
  
  # Trainings- und Testdatensätze
  train_data <- train[train_ind, ]
  test_data  <- train[test_ind, ]
  
  # Random Forest via ranger
  rf_model <- ranger(
    Listening_Time_minutes ~ Episode_Length_minutes + Guest_Popularity_percentage + Host_Popularity_percentage + Number_of_Ads,
    data       = train_data,
    num.trees  = 10,
    mtry       = floor(sqrt(4)), # number of trees
  )
  
  # Vorhersage auf Testdaten
  # Note: 'predict()' returns a list; '$predictions' is the actual numeric vector
  preds_obj <- predict(rf_model, data = test_data)
  preds     <- preds_obj$predictions
  
  # RMSE für diesen Fold
  actuals <- test_data$Listening_Time_minutes
  rmse    <- sqrt(mean((actuals - preds)^2))
  cv_rmse[i] <- rmse
  
  # Prüfen, ob dieses Modell besser ist als das bisher beste
  if (rmse < best_rmse) {
    best_rmse  <- rmse
    best_model <- rf_model
  }
}

# 3. Durchschnittlicher RMSE über alle Folds
mean_cv_rmse <- mean(cv_rmse)
mean_cv_rmse

# 'best_model' enthält jetzt das Modell mit dem geringsten RMSE 
# in seinem jeweiligen Fold.

# Instead of summary(best_model), just print it or inspect its elements:
best_model

```

## Gradient‐Boosted Decision Trees

```{r}
set.seed(123)      # For reproducible randomization
K <- 5             # Number of folds
N <- nrow(train)

# 1. Assign each row to one of K folds randomly
folds <- sample(rep(1:K, length.out = N))

# 2. Storage for RMSE in each fold
cv_rmse   <- numeric(K)

# Keep track of the best model across folds
best_model <- NULL
best_rmse  <- Inf

# XGBoost hyperparameters (tweak to your needs)
params <- list(
  objective = "reg:squarederror",
  max_depth = 6,
  eta       = 0.1
  # You can add other params here, e.g. subsample, colsample_bytree, etc.
)

# 3. Cross‐validation loop
for (i in 1:K) {
  # Training and test indices
  train_ind <- which(folds != i)
  test_ind  <- which(folds == i)
  
  # Subset your data
  train_data <- train[train_ind, ]
  test_data  <- train[test_ind, ]

  # Prepare matrices for XGBoost
  # -- Here we define X columns for training/test. 
  #    Adjust to match the predictors you want to use:
  X_train <- as.matrix(train_data[, c("Episode_Length_minutes",
                                      "Guest_Popularity_percentage",
                                      "Host_Popularity_percentage",
                                      "Number_of_Ads")])
  y_train <- train_data$Listening_Time_minutes
  
  X_test <- as.matrix(test_data[, c("Episode_Length_minutes",
                                    "Guest_Popularity_percentage",
                                    "Host_Popularity_percentage",
                                    "Number_of_Ads")])
  y_test <- test_data$Listening_Time_minutes
  
  # Convert to xgb.DMatrix
  dtrain <- xgb.DMatrix(data = X_train, label = y_train)
  dtest  <- xgb.DMatrix(data = X_test,  label = y_test)

  # Train XGBoost model
  # nrounds is how many boosting iterations (trees) you build
  xgb_model <- xgb.train(
    params  = params,
    data    = dtrain,
    nrounds = 100# Increase if you want to train more trees
  )
  
  # Predictions
  preds <- predict(xgb_model, newdata = dtest)
  
  # Compute RMSE
  rmse <- sqrt(mean((y_test - preds)^2))
  cv_rmse[i] <- rmse
  
  # Check if this fold's model is better
  if (rmse < best_rmse) {
    best_rmse  <- rmse
    best_model <- xgb_model
  }
}

# 4. Average RMSE across folds
mean_cv_rmse <- mean(cv_rmse)
mean_cv_rmse

# best_model is the XGBoost model that yielded the lowest RMSE in its fold
best_model


```
```{r}

```

### Feature Importance
```{r}
# 1. Importance als Datentabelle extrahieren
importance_table <- xgb.importance(model = best_model)
importance_table

# 2. (Optional) Plotten der Feature-Importance
xgb.plot.importance(importance_table)

```
### Final Model
```{r}
# Angenommen, train enthält dein gesamtes Trainingsdatenset.

# 1. Bereite alle Trainingsdaten für XGBoost vor
X_all <- as.matrix(train[, c("Episode_Length_minutes",
                             "Guest_Popularity_percentage",
                             "Host_Popularity_percentage",
                             "Number_of_Ads")])
y_all <- train$Listening_Time_minutes

# Erstelle das DMatrix-Objekt
dtrain_all <- xgb.DMatrix(data = X_all, label = y_all)

# 2. Trainiere das finale Modell auf allen Trainingsdaten
# Wir übernehmen dabei die Parameter und nrounds aus der CV-Phase
final_model <- xgb.train(
  params  = params,
  data    = dtrain_all,
  nrounds = 100   # Passe diesen Wert an, falls deine CV-Optimierung einen anderen Wert nahelegt
)

# Optional: Überprüfe die Performance des finalen Modells auf den Trainingsdaten
pred_all <- predict(final_model, newdata = dtrain_all)
rmse_all <- sqrt(mean((y_all - pred_all)^2))
print(paste("RMSE auf den Trainingsdaten:", rmse_all))

```


### Export

```{r}
# Convert relevant columns into a matrix
X_test <- as.matrix(test[, c("Episode_Length_minutes",
                             "Guest_Popularity_percentage",
                             "Host_Popularity_percentage",
                             "Number_of_Ads")])

# Create an xgb.DMatrix (no label since it's "test")
dtest <- xgb.DMatrix(data = X_test)


test_preds <- predict(best_model, newdata = dtest)
result <- data.frame(
  id         = test$id,
  Listening_Time_minutes = test_preds
)
head(result)


write.csv(result, "submission_brt.csv", row.names = FALSE)
```

```{r}
"Host_Popularity_percentage" "Publication_Day" "Publication_Time" "Guest_Popularity_percentage" "Number_of_Ads"           "Episode_Sentiment" "Listening_Time_minutes"     
```



